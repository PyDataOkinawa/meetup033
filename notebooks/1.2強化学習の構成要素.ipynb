{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# これから使うやつら"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 状態\n",
    "\n",
    "状態集合$S$  \n",
    "$S = {s_1, s_2, \\dots, s_N}$\n",
    "\n",
    "例えば2x1に黒丸をおくゲームがあるとしたら、2x1（マスの数）x2(黒丸,無し)　= 4の状態数がある\n",
    "\n",
    "時間ステップ$t$における状態を表す確率変数を$S_t$とする.\n",
    "\n",
    "0ステップから順に状態を並べると$S_0,S_1,S_2,\\dots,S_t,\\dots$とかける.  \n",
    "このそれぞれは$s_1,s_2,\\dots,s_N$のうちのいずれかを指します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行動\n",
    "\n",
    "行動集合$A(s)$  \n",
    "ある状態$s$において選択可能なすべての行動からのなる集合\n",
    "\n",
    "$A(s) = {a_1,a_2,\\dots,a_M}$\n",
    "\n",
    "この集合の要素を表す変数を$a$とする\n",
    "$(s)$が付いているのは状態によって要素の数が違う場合があるからです.\n",
    "\n",
    "時間ステップ$t$における行動を表す確率変数を$A_t$とする.\n",
    "$A_t$は、$A(S_t)$の要素のうち、いずれかの値をとる変数である.  \n",
    "0ステップから順に並べると$A_0,A_1,A_2,\\dots,A_t,\\dots$となる.\n",
    "\n",
    "このそれぞれは$a_1,a_2,\\dots,a_M$のいずれかを指します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 報酬\n",
    "\n",
    "$R$はすべての実数からなる集合とする. $S_t, A_t$および$S_{t+1}$に依存して定まる報酬を表す確率変数を$R_{t+1}$とする."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状態・行動・報酬の３つの変数を導入して、相互作用の内容を表す準備はおｋ．（相互作用って？ -> エージェントと環境がお互いにやりとりすること\n",
    "\n",
    "これらを使って、マルコフ決定過程モデルを見ていく"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "環境について、初期時刻における状態 -> 初期状態を確率的に決定し、これをエージェントに渡します.\n",
    "時間ステップ0における状態$S_0$を数式で表すと.\n",
    "\n",
    "$S_0 \\sim P_{0}(s)$\n",
    "\n",
    "こうかける. $S_0$は時刻0での状態を表す確率変数、$P_0$は初期状態分布である。記号$\\sim$は左側に書かれた確率変数が、右側に書かれた確率分布に従った独立分布である.（よくわからんが、それ以外の変数に依存しない.ということ）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マルコフ決定過程のモデルにおいて、次の状態は、現在の状態と行動によって確率的に決定される.その確率はエージェントが状態$s$において行動$a$を決定した時、状態が状態$s'$に遷移する確率として\n",
    "\n",
    "$P(s'|s,a)$\n",
    "\n",
    "で与えられる.\n",
    "\n",
    "例えば、$t+1$ステップ目における状態$S_{t+1}$は、$t$ステップ目の状態$S_t$と、その状態で絵選ばれた行動を$A_t$としたとき\n",
    "\n",
    "$S_{t+1} \\sim P(s'|S_{t}A_{t})$\n",
    "\n",
    "に、よって定まることとなる. このとき、$S_{t+1}$は、$S_{t-1}$や、$A_{t-1}$などには依存せず、$S_t$と$A_t$のみに依存して定まることに注意が必要である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 状態遷移確率\n",
    "\n",
    "マルコフ決定過程モデルにおいて、次の状態は現在の状態と行動によって、確率的に決定される。その確率はエージェントが状態sにおいて行動aを決定したときの状態が状態s'に遷移する確率して\n",
    "\n",
    "$ P(s'|s,a)$\n",
    "\n",
    "となる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 報酬関数\n",
    "\n",
    "$R_{t+1} = r(S_t, A_t, S_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方策\n",
    "\n",
    "エージェントが行動を決定するために利用する関数\n",
    "\n",
    "ある状態sにおける、ある行動aが選択される確率を\n",
    "\n",
    "$\\pi(a|s)$\n",
    "\n",
    "とおきます。\n",
    "強化学習は良い方策を求めたい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップとエピソード\n",
    "\n",
    "強化学習の世界で、時間が進むことを１時間ステップ進んだといいます。時間の単位です。時間という言葉がややこしいので今後１ステップということします。\n",
    "\n",
    "例えば、チェスのゲームがスタートして、ゲームに勝った、または負けたとします。タスクの開始から終了までをエピソードといいます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
